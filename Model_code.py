# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LKmWCVz9QAQIRae_sTRJL8LtxI6hcNuX
"""

import pandas as pd
import numpy as np
from keras.models import Model
from keras.layers import Input, LSTM, Dense, Embedding
from tensorflow import keras
import tensorflow as tf

PAD_token = 0   # Used for padding short sentences
SOS_token = 1   # Start-of-sentence token
EOS_token = 2   # End-of-sentence token
OOV_Token = 3
top_k = 1000
sentence_length = 68
batch_size = 64
latent_dim = 256
epochs = 100
embedding_size = 50
next_index1 = 4
next_index2 = 4

class Vocabulary:

    def __init__(self, name):

        self.name = name
        self.word2index = {"PAD":PAD_token, "SOS": SOS_token, "EOS": EOS_token, "OOV": OOV_Token}
        self.word2count = {}
        self.index2word = { PAD_token: "PAD", SOS_token: "SOS", EOS_token: "EOS",OOV_Token:"OOV" }
        self.num_words = 4
        self.num_sentences = 0
        self.longest_sentence = 0

    def add_word(self, word):
        if word not in self.word2index:
            # First entry of word into vocabulary
            self.word2index[word] = self.num_words
            self.word2count[word] = 1
            self.index2word[self.num_words] = word
            self.num_words += 1
        else:
            # Word exists; increase word count
            self.word2count[word] += 1

    def add_sentence(self, sentence):
        sentence_len = 0
        for word in sentence:
            sentence_len += 1
            self.add_word(word)
        if sentence_len > self.longest_sentence:
            # This is the longest sentence
            self.longest_sentence = sentence_len
        # Count the number of sentences
        self.num_sentences += 1

col_list1 = ["sourceLineTokens","targetLineTokens"]
df1 = pd.read_csv('D:\\IISc\\2nd Sem\\ASEML\\Assignment 2\\train.csv',usecols=col_list1)
df2 = pd.read_csv('D:\\IISc\\2nd Sem\\ASEML\\Assignment 2\\valid.csv',usecols=col_list1)
num_samples = len(df1.index)
num_samples_valid = len(df2.index)

def listcon1(df1,x,number):
    list1 = []
    for i in range(number):
        l = eval(df1[col_list1[x]].iloc[i])
        list1.append(l)
    return list1
list_source = listcon1(df1,0,num_samples)
list_target = listcon1(df1,1,num_samples)

def listcon2(df2,x,number):
    list1 = []
    for i in range(number):
        l = eval(df2[col_list1[x]].iloc[i])
        list1.append(l)
    return list1
list_source_valid = listcon2(df2,0,num_samples_valid)
list_target_valid = listcon2(df2,1,num_samples_valid)


voc1 = Vocabulary('name')
print(voc1)
for i in range(num_samples):
    voc1.add_sentence(list_source[i])
for i in range(num_samples):
    voc1.add_sentence(list_target[i])

def listcon1(l,num_samples):
    list1 = []
    for i in range(num_samples):
        x = ['SOS'] + l[i] + ['EOS']
        list1.append(x)
    return list1
list_source = listcon1(list_source,num_samples)
list_target = listcon1(list_target,num_samples)
list_source_valid = listcon1(list_source_valid,num_samples_valid)
list_target_valid = listcon1(list_target_valid,num_samples_valid)

print(list_target[0])
print(list_target_valid[0])

next_index1 = 4
sorted_age = sorted(voc1.word2count.items(), key = lambda kv: kv[1],reverse=True)
dictionary_of_tokens1 = { "PAD" : PAD_token, "SOS": SOS_token, "EOS": EOS_token, "OOV": OOV_Token}
for token in sorted_age:
    if(next_index1<=top_k+3):
        dictionary_of_tokens1[token[0]] = next_index1
        next_index1+=1
    else:
         dictionary_of_tokens1[token[0]] = OOV_Token

print(next_index1)

def tokanize(list1,number):
    token_list1 =[]
    for i in range(number):
        l = list1[i]
        ll = []
        for token in l:
            try:
                x = dictionary_of_tokens1[token]
            except KeyError:
                x = OOV_Token
            ll.append(x)
        token_list1.append(ll)
    return token_list1
token_list_source = tokanize(list_source,num_samples)
token_list_target = tokanize(list_target,num_samples)
token_list_source_valid = tokanize(list_source_valid,num_samples_valid)
token_list_target_valid = tokanize(list_target_valid,num_samples_valid)
           
print(list_source_valid[0],'\n',list_target_valid[0])  
print(token_list_source_valid[0],'\n',token_list_target_valid[0])

top_k = next_index1
print(top_k)

padded_target_token = tf.keras.preprocessing.sequence.pad_sequences(token_list_target,padding='post', maxlen = sentence_length)
padded_source_token = tf.keras.preprocessing.sequence.pad_sequences(token_list_source,padding='post', maxlen = sentence_length)
padded_target_token_valid = tf.keras.preprocessing.sequence.pad_sequences(token_list_target_valid,padding='post', maxlen = sentence_length)
padded_source_token_valid = tf.keras.preprocessing.sequence.pad_sequences(token_list_source_valid,padding='post', maxlen = sentence_length)
print(padded_source_token[0],top_k,next_index1)

encoder_input_data = np.zeros((len(padded_source_token), sentence_length),dtype='int32')
decoder_input_data = np.zeros((len(padded_source_token), sentence_length), dtype='int32')
decoder_target_data = np.zeros((len(padded_source_token), sentence_length, top_k),dtype='int32')

encoder_input_data_valid = np.zeros((len(padded_target_token_valid), sentence_length),dtype='int32')
decoder_input_data_valid = np.zeros((len(padded_target_token_valid), sentence_length), dtype='int32')
decoder_target_data_valid = np.zeros((len(padded_target_token_valid), sentence_length, top_k),dtype='int32')


print(encoder_input_data.shape, decoder_input_data.shape, decoder_target_data.shape,encoder_input_data.shape, decoder_input_data.shape, decoder_target_data.shape)

for i, (input_sent, target_sent) in enumerate(zip(padded_source_token, padded_target_token)):
    for t in range(len(input_sent)):
        encoder_input_data[i, t] = input_sent[t]
    for t in range(len(target_sent)):
        decoder_input_data[i, t] = target_sent[t]
        if t > 0:
            decoder_target_data[i, t - 1, target_sent[t]] = 1.

for i, (input_sent, target_sent) in enumerate(zip(padded_source_token_valid, padded_target_token_valid)):
    for t in range(len(input_sent)):
        encoder_input_data_valid[i, t] = input_sent[t]
    for t in range(len(target_sent)):
        decoder_input_data_valid[i, t] = target_sent[t]
        if t > 0:
            decoder_target_data_valid[i, t - 1, target_sent[t]] = 1.

# #Training Encoder
# encoder_inputs = Input(shape=(None,))
# encoder_embedding=  Embedding(top_k, embedding_size)(encoder_inputs)
# encoder_lstm = Bidirectional(LSTM(latent_dim, return_state=True))
# encoder_outputs, forward_h, forward_c, backward_h, backward_c =encoder_lstm(encoder_embedding)
# state_h = Concatenate()([forward_h, backward_h])
# state_c = Concatenate()([forward_c, backward_c])
# encoder_states = [state_h, state_c]

#Training Encoder
encoder_inputs = Input(shape=(None,))
encoder_embedding=  Embedding(top_k, embedding_size)(encoder_inputs)
encoder_lstm = LSTM(latent_dim, return_state=True)
encoder_outputs, state_h, state_c =encoder_lstm(encoder_embedding)
encoder_states = [state_h, state_c]

# #Training Decoder
# decoder_inputs = Input(shape=(None,))
# decoder_embedding=  Embedding(top_k, embedding_size)
# final_dex= decoder_embedding(decoder_inputs)
# decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)
# decoder_outputs, _, _, = decoder_lstm(final_dex,initial_state=encoder_states)
# decoder_dense = Dense(top_k, activation='softmax')
# decoder_outputs = decoder_dense(decoder_outputs)

#Training Decoder
decoder_inputs = Input(shape=(None,))
decoder_embedding=  Embedding(top_k, embedding_size)
final_dex= decoder_embedding(decoder_inputs)
decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
decoder_outputs, _, _, = decoder_lstm(final_dex,initial_state=encoder_states)
decoder_dense = Dense(top_k, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# Define the model that will turn
# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
# Compile & run training
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])

model.summary()

#Checkpoints
saved_checkpoints = keras.callbacks.ModelCheckpoint(
                                filepath='checkpointsachi/',
                                monitor= ['val_loss', 'acc'],
                                save_weights_only=False,
                                save_freq= 'epoch')

model.fit([encoder_input_data, decoder_input_data], decoder_target_data, 
          batch_size=batch_size, 
          epochs= epochs, 
          validation_data = ([encoder_input_data_valid, decoder_input_data_valid], decoder_target_data_valid)
          )

tf.keras.models.save_model(model,'D:\\IISc\\2nd Sem\\ASEML\\Assignment 2\\ASEMLDATA')

#Inference Encoder
encoder_model1 = Model(encoder_inputs, encoder_states)
encoder_model1.summary()

#Inference Decoder
decoder_state_input_h = Input(shape=(latent_dim,),name='name1')
decoder_state_input_c = Input(shape=(latent_dim,),name='name2')
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]
final_decoder_embedding= decoder_embedding(decoder_inputs)
decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_decoder_embedding, initial_state=decoder_states_inputs)
decoder_states2 = [state_h2, state_c2]
decoder_outputs2 = decoder_dense(decoder_outputs2)
decoder_model1 = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2)

rev_dictionary_of_tokens1 = dict([ (i,char) for char,i in dictionary_of_tokens1.items()])
#print(dictionary_of_tokens1)
#print(rev_dictionary_of_tokens1)
rev_dictionary_of_tokens1[3]='OOV_Token'
#print(rev_dictionary_of_tokens1)
def decode_sequence(input_seq):

    states_value = encoder_model1.predict(input_seq)
    target_seq = np.zeros((1,1))
    target_seq[0, 0] = dictionary_of_tokens1['SOS']

    stop_condition = False
    decoded_sentence = []
    while not stop_condition:
        output_tokens, h, c = decoder_model1.predict(
            [target_seq] + states_value)
        #print(output_tokens.shape)
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        #print(sampled_token_index)
        if(sampled_token_index==OOV_Token):
            sampled_char = 'OOV_Token'
        sampled_char = rev_dictionary_of_tokens1[sampled_token_index]
        #print(sampled_char)
        if (sampled_char == 'EOS' or len(decoded_sentence)>sentence_length):
          stop_condition = True
          break
        decoded_sentence.append(sampled_char)
        


        target_seq = np.zeros((1,1))
        target_seq[0, 0] = sampled_token_index

        # Update states
        states_value = [h, c]

    return decoded_sentence

import time
t0 = time.time()
acc = 0
for seq_index in range(num_samples_valid):

    input_seq = encoder_input_data_valid[seq_index: seq_index + 1]
    #print(input_seq)
    decoded_sentence = decode_sequence(input_seq)
    print(seq_index)
    x = list_target_valid[seq_index][1:-1]
    #print('Input sentence:', x)
    #print('Decoded sentence:', decoded_sentence)
    if(x == decoded_sentence):
      acc+=1
t1 = time.time() - t0
print(t1)

acc



